{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+eFN2tf77hfC+xJI/pkJq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJMwtBuJFho-","executionInfo":{"status":"ok","timestamp":1666230717960,"user_tz":300,"elapsed":27292,"user":{"displayName":"Jarod Peters","userId":"17481856514331319220"}},"outputId":"70abd63b-1293-4c6e-fdb3-25457dea57b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\r0% [Waiting for headers] [1 InRelease 14.2 kB/88.7 kB 16%] [Connected to cloud.\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [Waiting for headers] [1 InRelease 14.2 kB/88.7 kB 16%] [Connected to cloud.\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [1 InRelease 14.2 kB/88.7 kB\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","\r0% [2 InRelease gpgv 242 kB] [3 InRelease 14.2 kB/88.7 kB 16%] [1 InRelease 20.\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","\r0% [2 InRelease gpgv 242 kB] [3 InRelease 15.6 kB/88.7 kB 18%] [1 InRelease 25.\r0% [2 InRelease gpgv 242 kB] [3 InRelease 43.1 kB/88.7 kB 49%] [Waiting for hea\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Waiti\r                                                                               \rGet:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n","Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n","Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [95.2 kB]\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,035 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,554 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,208 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,467 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,248 kB]\n","Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,165 kB]\n","Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [983 kB]\n","Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,109 kB]\n","Fetched 17.5 MB in 5s (3,534 kB/s)\n","Reading package lists... Done\n"]}],"source":["import os\n","# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.0.3'\n","spark_version = 'spark-3.1.3'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"]},{"cell_type":"code","source":["!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8L8L2UfPFpQ4","executionInfo":{"status":"ok","timestamp":1666230729882,"user_tz":300,"elapsed":591,"user":{"displayName":"Jarod Peters","userId":"17481856514331319220"}},"outputId":"6d16601e-c0a5-4a37-ec3e-184081fce65d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-20 01:52:07--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n","Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n","Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1002883 (979K) [application/java-archive]\n","Saving to: ‘postgresql-42.2.16.jar’\n","\n","postgresql-42.2.16. 100%[===================>] 979.38K  5.46MB/s    in 0.2s    \n","\n","2022-10-20 01:52:08 (5.46 MB/s) - ‘postgresql-42.2.16.jar’ saved [1002883/1002883]\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"],"metadata":{"id":"ntVfSE70Fsi7","executionInfo":{"status":"ok","timestamp":1666230745713,"user_tz":300,"elapsed":8129,"user":{"displayName":"Jarod Peters","userId":"17481856514331319220"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Read in data from S3 Buckets\n","from pyspark import SparkFiles\n","url =\"https://databootcampjarodpeters-bucket.s3.amazonaws.com/user_data.csv\"\n","spark.sparkContext.addFile(url)\n","user_data_df = spark.read.csv(SparkFiles.get(\"user_data.csv\"), sep=\",\", header=True, inferSchema=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":885},"id":"-5u7LpBfFu1t","executionInfo":{"status":"error","timestamp":1666230879526,"user_tz":300,"elapsed":146,"user":{"displayName":"Jarod Peters","userId":"17481856514331319220"}},"outputId":"1745741a-0d7a-42bf-8e7f-a738b4d09d81"},"execution_count":6,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-cf4ed64fedd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"https://s3://databootcampjarodpeters-bucket.s3.amazonaws.com/user_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0muser_data_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkFiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36maddFile\u001b[0;34m(self, path, recursive)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \"\"\"\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddPyFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o34.addFile.\n: java.net.UnknownHostException: s3\n\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.base/java.net.Socket.connect(Socket.java:609)\n\tat java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:305)\n\tat java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)\n\tat java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:508)\n\tat java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:603)\n\tat java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)\n\tat java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)\n\tat java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:207)\n\tat java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1187)\n\tat java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1081)\n\tat java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:193)\n\tat java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:168)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:764)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:541)\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1627)\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1580)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}]},{"cell_type":"code","source":["# Show DataFrame\n","user_data_df.show()"],"metadata":{"id":"IJa9E8pBFwtc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url =\"https://YOUR-BUCKET-NAME.s3.amazonaws.com/user_payment.csv\"\n","spark.sparkContext.addFile(url)\n","user_payment_df = spark.read.csv(SparkFiles.get(\"user_payment.csv\"), sep=\",\", header=True, inferSchema=True)\n","\n","# Show DataFrame\n","user_payment_df.show()"],"metadata":{"id":"uyQtt_A9Fxbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Store environmental variable\n","from getpass import getpass\n","password = getpass('Enter database password')\n","# Configure settings for RDS\n","mode = \"append\"\n","jdbc_url=\"jdbc:postgresql://<connection string>:5432/<database-name>\"\n","config = {\"user\":\"postgres\",\n","          \"password\": password,\n","          \"driver\":\"org.postgresql.Driver\"}"],"metadata":{"id":"KwDeRcBrF0k0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write DataFrame to active_user table in RDS\n","clean_user_df.write.jdbc(url=jdbc_url, table='active_user', mode=mode, properties=config)"],"metadata":{"id":"pL5Scf45Hg8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write dataframe to billing_info table in RDS\n","clean_billing_df.write.jdbc(url=jdbc_url, table='billing_info', mode=mode, properties=config)"],"metadata":{"id":"utUBYWkkHjPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write dataframe to payment_info table in RDS\n","clean_payment_df.write.jdbc(url=jdbc_url, table='payment_info', mode=mode, properties=config)"],"metadata":{"id":"7r4BBWOQHlB7"},"execution_count":null,"outputs":[]}]}